add_model(lm_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
#splits     <- initial_split(data, strata = Class, prop = 0.80)
#data_test  <- testing(splits)
#data_train <- training(splits)
data_val   <- validation_split(data, strata = Class, prop = 0.90) #CV
recipe <- recipe(Class ~ ., data = data) |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
?logistic_reg
lr_mod <- logistic_reg(penalty = tune(), mixture = 0.5) |>
set_engine("glmnet")
lr_res <- workflow() |>
add_model(lr_mod) |>
add_recipe(recipe) |>
tune_grid(resamples = data_val,
grid = tibble(penalty = 10^seq(-5, -1, length.out = 30)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
lr_best <- lr_res |> select_best("roc_auc")
View(lr_best)
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine("ranger", num.threads = 2) %>%
set_mode("classification")
rf_res <- workflow() |>
add_model(rf_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
rf_best <- rf_res %>% select_best(metric = "roc_auc")
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
rf2_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine("rf", num.threads = 2) %>%
set_mode("classification")
rf2_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine("rf", num.threads = 2) %>%
set_mode("classification")
show_engines("rand_forest")
rf2_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine("randomForest", num.threads = 2) %>%
set_mode("classification")
rf2_res <- workflow() |>
add_model(rf2_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
install.packages("randomForest")
rf2_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine("randomForest", num.threads = 2) %>%
set_mode("classification")
rf2_res <- workflow() |>
add_model(rf2_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
rf2_best <- rf_res %>% select_best(metric = "roc_auc")
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
rf2_res |> show_best(n=1)
set_engine("LiblineaR", num.threads = 2) %>%
set_mode("classification")
rf2_mod <- svm_linear(mode = "unknown", cost = NULL, margin = NULL) |>
set_engine("LiblineaR", num.threads = 2) %>%
set_mode("classification")
rf2_res <- workflow() |>
add_model(rf2_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
install.packages("LiblineaR")
rf2_mod <- svm_linear(mode = "unknown", cost = NULL, margin = NULL) |>
set_engine("LiblineaR", num.threads = 2) %>%
set_mode("classification")
rf2_res <- workflow() |>
add_model(rf2_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
rf2_best <- rf_res %>% select_best(metric = "roc_auc")
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
rf2_res |> show_best(n=1)
rf2_mod <- svm_linear(cost = NULL, margin = NULL) |>
set_engine("LiblineaR", num.threads = 2) %>%
set_mode("classification")
rf2_res <- workflow() |>
add_model(rf2_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
rf2_mod <- svm_linear(cost = tune(), margin = NULL) |>
set_engine("LiblineaR", num.threads = 2) %>%
set_mode("classification")
rf2_res <- workflow() |>
add_model(rf2_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
parsnip_addin()
#splits     <- initial_split(data, strata = Class, prop = 0.80)
#data_test  <- testing(splits)
#data_train <- training(splits)
data_val   <- validation_split(data, strata = Class, prop = 0.80) #CV
library(tidymodels)
set.seed(2)
data <- read_csv("C:/Users/upc6/Desktop/prediction_course/train.csv", show_col_types = F) |>
dplyr::select(Quality:Class) |>
mutate(Class = factor(Class, levels = c(1,0),labels=c(1,0)))
#splits     <- initial_split(data, strata = Class, prop = 0.80)
#data_test  <- testing(splits)
#data_train <- training(splits)
data_val   <- validation_split(data, strata = Class, prop = 0.80) #CV
recipe <- recipe(Class ~ ., data = data) |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
recipe <- recipe(Class ~ ., data = data) |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
View(data_val)
lr_mod <- logistic_reg(penalty = tune(), mixture = 0.5) |>
set_engine("glmnet")
lr_res <- workflow() |>
add_model(lr_mod) |>
add_recipe(recipe) |>
tune_grid(resamples = data_val,
grid = tibble(penalty = 10^seq(-5, -1, length.out = 30)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
lr_best <- lr_res |> select_best("roc_auc")
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
bo_mod <- boost_tree(tree_depth = tune(), trees = tune(), learn_rate = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
bo_res <- workflow() |>
add_model(bo_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
db_res <- workflow() |>
add_model(db_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
install.packages("dbarts")
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
db_res <- workflow() |>
add_model(db_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
db_best <- db_res %>% select_best(metric = "roc_auc")
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
bo_res |> show_best(n=1)
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
db_res |> show_best(n=1)
parsnip_addin()
xg_mod <- boost_tree(tree_depth = tune(), trees = tune(), learn_rate = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
xg_res <- workflow() |>
add_model(xg_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
xg_mod <- boost_tree(tree_depth = tune(), trees = tune(), learn_rate = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
xg_res <- workflow() |>
add_model(xg_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
install.packages("xgboost")
xg_mod <- boost_tree(tree_depth = tune(), trees = tune(), learn_rate = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
xg_res <- workflow() |>
add_model(xg_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
xg_best <- xg_res %>% select_best(metric = "roc_auc")
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
db_res |> show_best(n=1)
xg_res |> show_best(n=1)
sv_mod <- svm_linear(cost = tune(), margin = tune()) %>%
set_engine('LiblineaR') %>%
set_mode('classification')
sv_res <- workflow() |>
add_model(sv_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
sv_best <- sv_res %>% select_best(metric = "roc_auc")
sv_mod <- svm_linear(cost = tune(), margin = tune(), type="class") %>%
set_engine('LiblineaR') %>%
set_mode('classification')
sv_mod <- svm_linear(cost = tune(), margin = tune()) %>%
set_engine('LiblineaR') %>%
set_mode('classification')
sv_res <- workflow() |>
add_model(sv_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
parsnip_addin()
install.packages("kernlab")
sv_mod <- svm_linear(cost = tune(), margin = tune()) %>%
set_engine('kernlab') %>%
set_mode('classification')
sv_res <- workflow() |>
add_model(sv_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
sv_best <- sv_res %>% select_best(metric = "roc_auc")
svm_linear_kernlab_spec <-
svm_linear(cost = tune(), margin = tune()) %>%
set_engine('kernlab') %>%
set_mode('classification')
lr_res |> show_best(n=1)
rf_res |> show_best(n=1)
db_res |> show_best(n=1)
xg_res |> show_best(n=1)
sv_res |> show_best(n=1)
parsnip_addin()
install.packages("keras")
ke_mod <- mlp(hidden_units = tune(), penalty = tune(), dropout = tune(), epochs = tune(), activation = tune()) %>%
set_engine('keras') %>%
set_mode('classification')
ke_res <- workflow() |>
add_model(ke_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
ke_best <- ke_res %>% select_best(metric = "roc_auc")
show_notes(.Last.tune.result)
ke_mod <- mlp(hidden_units = tune(), epochs = tune(), activation = tune()) %>%
set_engine('keras') %>%
set_mode('classification')
ke_res <- workflow() |>
add_model(ke_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
ke_best <- ke_res %>% select_best(metric = "roc_auc")
ke_mod <- mlp(hidden_units = tune(), epochs = tune(), activation = "relu") %>%
set_engine('keras') %>%
set_mode('classification')
ke_res <- workflow() |>
add_model(ke_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
ke_best <- ke_res %>% select_best(metric = "roc_auc")
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc)
tune_grid(data_val,
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc)
tune_grid(data_val,
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
?tune_grid
ke_mod <- mlp(hidden_units = tune(), epochs = tune(), activation = "relu") %>%
set_engine('keras') %>%
set_mode('classification')
ke_res <- workflow() |>
add_model(ke_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
ke_best <- ke_res %>% select_best(metric = "roc_auc")
install_keras()
library(tidymodels)
set.seed(2)
data <- read_csv("C:/Users/upc6/Desktop/prediction_course/train.csv", show_col_types = F) |>
dplyr::select(Quality:Class) |>
mutate(Class = factor(Class, levels = c(1,0),labels=c(1,0)))
#splits     <- initial_split(data, strata = Class, prop = 0.80)
#data_test  <- testing(splits)
#data_train <- training(splits)
data_val   <- validation_split(data, strata = Class, prop = 0.80) #CV
recipe <- recipe(Class ~ ., data = data) |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
library(tidymodels)
set.seed(2)
data <- read_csv("C:/Users/upc6/Desktop/prediction_course/train.csv", show_col_types = F) |>
dplyr::select(Quality:Class) |>
mutate(Class = factor(Class, levels = c(1,0),labels=c(1,0)))
#splits     <- initial_split(data, strata = Class, prop = 0.80)
#data_test  <- testing(splits)
#data_train <- training(splits)
data_val   <- validation_split(data, strata = Class, prop = 0.80) #CV
recipe <- recipe(Class ~ ., data = data) |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
lr_mod <- logistic_reg(penalty = tune(), mixture = 0.5) |>
set_engine("glmnet")
lr_res <- workflow() |>
add_model(lr_mod) |>
add_recipe(recipe) |>
tune_grid(resamples = data_val,
grid = tibble(penalty = 10^seq(-5, -1, length.out = 30)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
lr_best <- lr_res |> select_best("roc_auc")
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine("ranger", num.threads = 2) %>%
set_mode("classification")
rf_res <- workflow() |>
add_model(rf_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
rf_best <- rf_res %>% select_best(metric = "roc_auc")
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
library(dbdats)
library(dbarts)
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
xg_mod <- boost_tree(tree_depth = tune(), trees = tune(), learn_rate = tune(), min_n = tune(),
loss_reduction = tune(), sample_size = tune(), stop_iter = tune()) %>%
set_engine('xgboost') %>%
set_mode('classification')
xg_res <- workflow() |>
add_model(xg_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
xg_best <- xg_res %>% select_best(metric = "roc_auc")
sv_mod <- svm_linear(cost = tune(), margin = tune()) %>%
set_engine('kernlab') %>%
set_mode('classification')
sv_res <- workflow() |>
add_model(sv_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
sv_best <- sv_res %>% select_best(metric = "roc_auc")
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune())
parsnip_addin()
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
library(tidymodels)
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
library(tidyverse)
library(yardstick)
train <- read_csv("C:/Users/upc6/Desktop/prediction_course/train.csv") |>
mutate(pclass = factor(pred_class, levels=c(1,0), labels=c(1,0)),
tclass = factor(Class, levels=c(1,0), labels=c(1,0)))
library(tidyverse)
library(tidymodels)
train <- read_csv("C:/Users/upc6/Desktop/prediction_course/train.csv") |>
mutate(pclass = factor(pred_class, levels=c(1,0), labels=c(1,0)),
tclass = factor(Class, levels=c(1,0), labels=c(1,0)))
library(tidymodels)
set.seed(2)
data <- read_csv("C:/Users/upc6/Desktop/prediction_course/train.csv", show_col_types = F) |>
dplyr::select(Quality:Class) |>
mutate(Class = factor(Class, levels = c(1,0),labels=c(1,0)))
#splits     <- initial_split(data, strata = Class, prop = 0.80)
#data_test  <- testing(splits)
#data_train <- training(splits)
data_val   <- validation_split(data, strata = Class, prop = 0.80) #CV
recipe <- recipe(Class ~ ., data = data) |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
step_normalize(all_predictors())
lr_mod <- logistic_reg(penalty = tune(), mixture = 0.5) |>
set_engine("glmnet")
lr_res <- workflow() |>
add_model(lr_mod) |>
add_recipe(recipe) |>
tune_grid(resamples = data_val,
grid = tibble(penalty = 10^seq(-5, -1, length.out = 30)),
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
lr_best <- lr_res |> select_best("roc_auc")
rf_mod <- rand_forest(mtry = tune(), min_n = tune(), trees = 500) %>%
set_engine("ranger", num.threads = 2) %>%
set_mode("classification")
rf_res <- workflow() |>
add_model(rf_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
rf_best <- rf_res %>% select_best(metric = "roc_auc")
db_mod <- bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(),
prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
db_res <- workflow() |>
add_model(db_mod) |>
add_recipe(recipe) |>
tune_grid(data_val,
grid = 25,
control = control_grid(save_pred = TRUE),
metrics = metric_set(roc_auc))
db_best <- db_res %>% select_best(metric = "roc_auc")
bart_dbarts_spec <-
bart(trees = tune(), prior_terminal_node_coef = tune(), prior_terminal_node_expo = tune(), prior_outcome_range = tune()) %>%
set_engine('dbarts') %>%
set_mode('classification')
